from cc_json import lexer_tokens
import re


class GetToken:
    """
    A class representing a token generated by the lexer.

    Attributes:
        type (str): The type of the token.
        position (int): The position of the token in the input text.
        value (str): The value of the token.
    """
    def __init__(self, token_type, token_value, curr_position):
        self.type = token_type
        self.position = curr_position
        self.value = token_value


class Lexer:
    """
    A lexer class for tokenizing input text into tokens.

    Attributes:
        text (str): The input text to be tokenized.
        position (int): The current position in the input text.
        tokens (list): A list to store generated tokens.
    """
    def __init__(self, input_text=None):
        """
        Initializes the Lexer with optional input text.

        Args:
            input_text (str, optional): The input text to be tokenized. Defaults to None.
        """
        self.text = input_text
        self.position = 0
        self.tokens = []

    def tokenize(self, file_path=None):
        """
        Tokenizes the input text or text from a file.

        Args:
            file_path (str, optional): The path to the file to be tokenized. Defaults to None.

        Returns:
            list: A list of generated tokens.
        """

        if file_path:
            try:
                with open(file_path, 'r') as file:
                    self.text = file.read()
            except FileNotFoundError:
                print(f"Error: File not found - {file_path}")
                return None

        while self.position < len(self.text):
            self.get_next_token()
        return self.tokens

    def get_next_token(self):
        """
        Generates the next token by matching patterns against the input text.

        Raises:
            ValueError: If no valid token pattern is matched.
        """
        for token_type, rgx in lexer_tokens.Token_Definitions:
            print(f"Checking token type: {token_type}")
            pattern = re.compile(rgx)
            match = pattern.match(self.text, pos=self.position)
            if match:
                value = match.group()
                print(f"Matched value: {value}")
                self.position += len(value)
                token = GetToken(token_type, value, self.position)
                self.tokens.append(token)
                print("Generated token:", token_type, value)
                return
        raise ValueError("Unrecognized token")

