from lexer import Lexer


# parser.py - Parses the tokens generated by the lexer and constructs the JSON data structure


class JSONParser:
    def __init__(self):
        self.tokens = []
        self.current_token = None
        self.current_index = 0

    def parse(self, tokens):
        self.tokens = tokens
        self.current_index = 0
        self.current_token = self.tokens[self.current_index]

        return self.parse_json()

    def consume_token(self):
        self.current_index += 1
        if self.current_index < len(self.tokens):
            self.current_token = self.tokens[self.current_index]
        else:
            self.current_token = None

    # This method ensures that the current token matches the expected types before proceeding with parsing.
    def validate_token(self, expected_type):
        if self.current_token and self.current_token.type == expected_type:
            self.consume_token()  # Calls the tokenize method in the lexer class which moves position and tokenizes each
            # char
        else:
            raise ValueError(f"Expected token of type '{expected_type}' but got '{self.current_token.type}'")
